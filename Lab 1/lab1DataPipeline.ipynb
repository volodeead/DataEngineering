{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "z4hXX67KaCjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd40d551-b5cf-4760-ffde-bc620bd9f1fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=3c5b79a0242040925eed11516d01d19fc9f399e424b348ff854e7cdd7e0ae424\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "js2xtlBjYj_t"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split, size, expr, concat_ws\n",
        "from pyspark.ml.feature import NGram, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "import re\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"GitHubNGrams\").getOrCreate()\n",
        "\n",
        "# Load the JSON data into a DataFrame\n",
        "json_data = spark.read.json(\"10K.github.jsonl\")\n",
        "\n",
        "# Filter data for PushEvent type\n",
        "push_events = json_data.filter(col(\"type\") == \"PushEvent\")\n",
        "\n",
        "# Define a function to extract and preprocess commit messages\n",
        "def process_commit_messages(commit_messages):\n",
        "    # Convert to lowercase and remove punctuation and underscores\n",
        "    cleaned_messages = [re.sub(r'\\W+', ' ', msg.lower()) for msg in commit_messages]\n",
        "    return ' '.join(cleaned_messages)\n",
        "\n",
        "# UDF for processing commit messages\n",
        "process_commit_messages_udf = spark.udf.register(\"process_commit_messages\", process_commit_messages)\n",
        "\n",
        "# Apply transformations to the DataFrame\n",
        "processed_data = push_events.withColumn(\n",
        "    \"processed_commits\",\n",
        "    process_commit_messages_udf(col(\"payload.commits.message\"))\n",
        ")\n",
        "\n",
        "# Tokenize and apply NGram transformation\n",
        "tokenizer = Tokenizer(inputCol=\"processed_commits\", outputCol=\"tokenized_words\")\n",
        "ngram = NGram(n=3, inputCol=tokenizer.getOutputCol(), outputCol=\"ngrams_result\")\n",
        "pipeline = Pipeline(stages=[tokenizer, ngram])\n",
        "model = pipeline.fit(processed_data)\n",
        "result = model.transform(processed_data)\n",
        "\n",
        "# Extract only the first five words from the n-grams\n",
        "result = result.withColumn(\n",
        "    \"first_five_words\",\n",
        "    expr(\"slice(ngrams_result, 1, case when size(ngrams_result) >= 5 then 5 else size(ngrams_result) end)\")\n",
        ")\n",
        "\n",
        "# Handle cases where 1-2 words are present\n",
        "result = result.withColumn(\n",
        "    \"first_five_words\",\n",
        "    expr(\"case when size(tokenized_words) <= 2 then tokenized_words else first_five_words end\")\n",
        ")\n",
        "\n",
        "# Convert the array of strings to a single string\n",
        "result = result.withColumn(\"first_five_words\", concat_ws(\", \", col(\"first_five_words\")))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "result.select(\"actor.display_login\", \"first_five_words\").coalesce(1).write.csv(\"output.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ]
}